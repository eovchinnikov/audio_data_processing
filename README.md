# Отчёт по Сбору и Фильтрации Аудиоданных

## Цели Проекта

- Собрать качественный аудиодатасет объёмом не менее **50 часов**.
- Обеспечить баланс по полу говорящих:
  - Минимум **20 часов** записей с мужским голосом.
  - Минимум **20 часов** записей с женским голосом.
- Отобрать аудиофайлы с частотой дискретизации **не ниже 16 kHz**.
- Исключить записи с фоновыми шумами и одновременной речью (перекрывающимися голосами).
- Обеспечить высокий уровень чистоты аудио за счёт грамотного подбора исходных данных без применения сложных технологий шумоподавления.
- Обеспечить воспроизводимость результатов и подготовить чистую структуру итогового датасета для последующего анализа и обучения моделей.

---

## Используемые Источники Данных

- **Основной датасет:** Mozilla Common Voice v20.0 (русский язык).
- **Размер исходного датасета:** ~6.3 GB.
- Основной задачей было выбрать размеченный датасет с информацией о поле говорящих.
- Данный ресурс был выбран после предварительного анализа известных открытых аудиодатасетов.
  Он обладает следующими преимуществами:
  - Наличие размеченных метаданных, включая информацию о поле и возрасте говорящих.
  - Данные представлены в удобном структурированном виде, что упрощает их обработку.
- Для обеспечения достижения целевых метрик по качеству и балансу данных был выбран именно этот датасет, содержащий около **281 часа аудиозаписей**.  
  Предварительная грубая оценка потерь при фильтрации показала, что даже после удаления шумных и невалидных записей гарантированно можно получить не менее **20 часов** мужского и **20 часов** женского голоса.

---

## Этапы Обработки Данных

### 1. Предобработка Метаданных
- Загрузка метаданных из файла `validated.tsv`.
- Фильтрация записей по наличию указания пола говорящих.
- Преобразование пола в категории `male` и `female`.

### 2. Фильтрация Аудиофайлов
- Исключение файлов с частотой дискретизации ниже **16 kHz** и длительностью менее **1 секунды**.
- Применение модели Voice Activity Detection (VAD) для исключения аудио без речи.
- Отбрасывание файлов с фоновыми шумами и одновременной речью.
- Достижение баланса по полу на уровне заданных лимитов.

### 3. Сохранение Валидных Данных
- Копирование валидных аудиофайлов в отдельную директорию:  
  `data/processed/valid_clips/`.
- Очистка директории перед каждым запуском для обеспечения чистоты данных.
- Сохранение актуальных метаданных в CSV-файл:  
  `data/processed/metadata.csv`.

---

## Итоговая Структура Данных

| Колонка       | Описание                                |
|----------------|-----------------------------------------|
| `file_path`    | Путь к валидному аудиофайлу.             |
| `sentence`     | Текст, произнесённый в аудиофайле.       |
| `gender`       | Пол говорящего (`male` / `female`).      |
| `duration_sec` | Длительность аудиофайла в секундах.      |
| `age`          | Возраст говорящего (если доступен).      |

---

## Результаты

| Параметр             | Значение     |
|----------------------|--------------|
| Мужской голос        | 25.00 часов  |
| Женский голос        | 25.00 часов  |
| Суммарное время      | 50.00 часов  |
| Частота дискретизации| ≥16 kHz      |
| Количество файлов    | 34310        |
| Время выполнения     | 19.08 минут  |

---

## Оценка Времени Сбора При Масштабировании

- При текущем подходии без оптимизации, линейная оценка показывает, что
обработка датасета объёмом 50,000 часов займет около 13 дней,
а обработка датасета объёмом 500,000 часов займет около 130 дней.

Возможные способы оптимизации:

1) Чтобы ускорить процесс обработки датасета, можно на первом этапе только идентифицировать валидные файлы, а копировать их в отдельную папку позже, после завершения основной фильтрации.  
   - Это позволяет уменьшить количество операций записи на диск, что является узким местом в производительности.  
   - Пример измерений:
     - С копированием файлов во время обработки: ~19 минут.
     - Без копирования (только отбор файлов): ~13 минут.

2) Многопоточная и многопроцессная обработка**  
   - Распараллеливание загрузки и анализа файлов позволяет в 4–8 раз ускорить обработку даже на обычном ноутбуке с 8 потоками.

3) Перенос вычислений в облако
   - При использовании облачных сервисов и выделенных мощных серверов (например, с 64 ядрами) время обработки можно сократить в десятки раз.

Таким образом, можно грубо оценить, что оптимизированный подход позволит 
обработать датасет объёмом 50,000 часов за ~ 2-3 дня, а датасет объёмом 500,000 
часов за ~ 2-3 недели.

Также необходимо учитывать время, затраченное на поиск и загрузку качественных датасетов.

---